{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coden\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\coden\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# for keras\n",
    "from classification_models.keras import Classifiers\n",
    "\n",
    "# model\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# image processing, callbacks\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# file\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# sub\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plot\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 연결 확인 및 할당 메모리 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12646702193184146134\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9383706624\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6708710404353872730\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b3:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 10)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 8번의 학습 및 비교 분석 진행\n",
    "- 해당 ipynb파일에선 가장 좋은 방식인 2-4차의 학습 진행 코드를 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Vision Transformer\n",
    "from vit_keras import vit\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 폴더 경로 설정\n",
    "train_data_dir = './Kfood/Kfood/kfood_health_train/'  # 학습용 데이터 폴더 경로\n",
    "val_data_dir = './Kfood/Kfood/kfood_health_val/' # 검증용 데이터 폴더 경로\n",
    "\n",
    "all_items = os.listdir(train_data_dir)\n",
    "num_classes = len(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic_augmentation(image):\n",
    "    # 이미지를 4등분하여 각 부분에 모자이크를 적용\n",
    "    height, width, _ = image.shape\n",
    "    quarter_height, quarter_width = height // 2, width // 2\n",
    "\n",
    "    # 랜덤한 위치에서 시작점을 선택\n",
    "    start_x = np.random.randint(0, quarter_height)\n",
    "    start_y = np.random.randint(0, quarter_width)\n",
    "\n",
    "    # 모자이크할 영역 선택\n",
    "    end_x = start_x + quarter_height\n",
    "    end_y = start_y + quarter_width\n",
    "\n",
    "    # 모자이크 적용\n",
    "    image[start_x:end_x, start_y:end_y, :] = np.mean(image[start_x:end_x, start_y:end_y, :], axis=(0, 1), keepdims=True)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 크기 설정\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,  # 이미지 값을 0과 1 사이로 정규화\n",
    "    \n",
    "    rotation_range=20,  # 회전\n",
    "    \n",
    "    width_shift_range=0.2,  # 가로 이동\n",
    "    height_shift_range=0.2,  # 세로 이동\n",
    "    \n",
    "    shear_range=0.2,  # 전단 변형\n",
    "    \n",
    "    horizontal_flip=True,  # 수평 뒤집기\n",
    "    vertical_flip=True,  # 수직 뒤집기\n",
    "    \n",
    "    channel_shift_range=40,  # 채널 시프트\n",
    "    zoom_range=0.2,  # 확대/축소\n",
    "    # brightness_range=[0.8, 1.2]\n",
    "    preprocessing_function=mosaic_augmentation,  # 모자이크 적용 함수\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255, # 이미지 값을 0과 1 사이로 정규화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14115 images belonging to 13 classes.\n",
      "Found 1764 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# 학습용 데이터 로딩 및 전처리\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical', # 다중 클래스 분류를 위해 categorical로 설정  \n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = vit.vit_b32(\n",
    "            image_size = 224,\n",
    "            activation = 'softmax',\n",
    "            pretrained = False,   \n",
    "            include_top = False,\n",
    "            pretrained_top = False,\n",
    "            classes = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(vit_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(13, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
    "              metrics = [tf.keras.metrics.TopKCategoricalAccuracy(k=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 파일 경로 및 설정\n",
    "checkpoint_filepath = './Checkpoint/Mission3_VIT_zoom.h5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,  # 최상의 모델만 저장\n",
    "    monitor='val_top_k_categorical_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # 검증 손실을 모니터링\n",
    "    factor=0.2,  # 학습률을 0.2배로 줄임\n",
    "    patience=5,  # 5 에폭 동안 검증 손실이 감소하지 않으면 학습률을 조절\n",
    "    min_lr=1e-6  # 학습률의 하한 설정\n",
    ")\n",
    "\n",
    "# 모델을 학습할 때 ModelCheckpoint 콜백을 지정\n",
    "with tf.device('/device:GPU:0'): \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        verbose=1,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[model_checkpoint_callback, reduce_lr_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검증 데이터에 대한 평가 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1764 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# 체크포인트 파일 경로\n",
    "checkpoint_filepath = './Checkpoint/Mission3_VIT_zoom.h5'\n",
    "\n",
    "# 모델 로드\n",
    "loaded_model = load_model(checkpoint_filepath)\n",
    "\n",
    "# 검증용 데이터 제너레이터 설정\n",
    "val_data_dir = './Kfood/Kfood/kfood_health_val/' \n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255) \n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764/1764 [==============================] - 92s 50ms/step\n",
      "              precision    recall  f1-score      support\n",
      "가리비            0.877551  0.834951  0.855721   103.000000\n",
      "갈비찜            0.788462  0.814570  0.801303   151.000000\n",
      "고등어            0.915493  0.948905  0.931900   137.000000\n",
      "김치국            0.946746  0.969697  0.958084   165.000000\n",
      "낚지볶음           0.958333  0.942623  0.950413   122.000000\n",
      "돼지갈비찜          0.807339  0.822430  0.814815   107.000000\n",
      "된장찌개           0.916667  0.908257  0.912442   109.000000\n",
      "떡국             0.943548  0.991525  0.966942   118.000000\n",
      "모듬초밥           0.985714  0.857143  0.916944   161.000000\n",
      "배추김치           0.938462  0.953125  0.945736   128.000000\n",
      "부대찌개           0.925926  0.930233  0.928074   215.000000\n",
      "순대             0.884211  0.965517  0.923077    87.000000\n",
      "오리로스구이         0.987261  0.962733  0.974843   161.000000\n",
      "accuracy       0.916667  0.916667  0.916667     0.916667\n",
      "macro avg      0.913516  0.915516  0.913869  1764.000000\n",
      "weighted avg   0.918033  0.916667  0.916699  1764.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with tf.device('/device:GPU:0'): \n",
    "    validation_predict = loaded_model.predict(validation_generator)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "validation_pred_classes = np.argmax(validation_predict,axis=1)\n",
    "\n",
    "# classification report 출력\n",
    "target_names = list(validation_generator.class_indices.keys())\n",
    "\n",
    "\n",
    "validation_results = pd.DataFrame(classification_report(validation_generator.classes, validation_pred_classes, target_names=target_names, output_dict=True)).transpose()\n",
    "print(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델에서 Attention 레이어를 추출\n",
    "attention_layer = vit_model.get_layer(name='attention')\n",
    "\n",
    "# 샘플 이미지 경로 설정\n",
    "sample_image_path = 'path/to/your/sample/image.jpg'\n",
    "\n",
    "# 샘플 이미지를 모델의 입력 크기로 로드 및 전처리\n",
    "sample_image = cv2.imread(sample_image_path)\n",
    "sample_image = cv2.resize(sample_image, (img_width, img_height))\n",
    "sample_image = sample_image / 255.0  # 이미지를 0과 1 사이로 정규화\n",
    "sample_image = np.expand_dims(sample_image, axis=0)  # 배치 차원 추가\n",
    "\n",
    "# 모델의 입력에 대한 Attention Map을 얻음\n",
    "attention_map = attention_layer.predict(sample_image)\n",
    "\n",
    "# Attention Map을 히트맵으로 변환\n",
    "heatmap = np.mean(attention_map, axis=-1)  # 각 Head의 평균을 구함\n",
    "heatmap = np.maximum(heatmap, 0)  # ReLU를 적용하여 음수 값을 제거\n",
    "heatmap /= np.max(heatmap)  # 정규화\n",
    "\n",
    "# 샘플 이미지 로드\n",
    "original_image = cv2.imread(sample_image_path)\n",
    "original_image = cv2.resize(original_image, (img_width, img_height))\n",
    "\n",
    "# 히트맵을 원본 이미지 크기로 조정\n",
    "heatmap = cv2.resize(heatmap[0], (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "# 히트맵을 RGB 형식으로 변환\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "# 히트맵을 원본 이미지에 적용하여 시각화\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.4 + original_image  # 이미지를 40%만큼 투명하게 함\n",
    "\n",
    "# 결과 시각화\n",
    "plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
